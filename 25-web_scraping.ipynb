{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### CSCI 303\n",
    "# Introduction to Data Science\n",
    "<p/>\n",
    "### Web Scraping\n",
    "\n",
    "Credits: Many thanks to Zachary Smialek for this Lecture/Notebook Idea!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Lecture\n",
    "---\n",
    "- Overview of HTML\n",
    "- Useful Libraries\n",
    "- What is Scraping and Web Crawling\n",
    "- Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background\n",
    "---\n",
    "\n",
    "HTML (HyperText Markup Language) is the fundmental building block of the Web. The purpose of HTML is describe to a webpage's appearance. HTML is context free grammer makes parsing a bit different. Quick note regular expressions will not do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What HTML looks like"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<HTML>\n",
    "<HEAD>\n",
    "<TITLE>Your Title Here</TITLE>\n",
    "</HEAD>\n",
    "<BODY BGCOLOR=\"FFFFFF\">\n",
    "<CENTER><IMG SRC=\"clouds.jpg\" ALIGN=\"BOTTOM\"> </CENTER>\n",
    "<HR>\n",
    "<a href=\"http://somegreatsite.com\">Link Name</a>\n",
    "is a link to another nifty site\n",
    "<H1>This is a Header</H1>\n",
    "<H2>This is a Medium Header</H2>\n",
    "Send me mail at <a href=\"mailto:support@yourcompany.com\">\n",
    "support@yourcompany.com</a>.\n",
    "<P> This is a new paragraph!\n",
    "<P> <B>This is a new paragraph!</B>\n",
    "<BR> <B><I>This is a new sentence without a paragraph break, in bold italics.</I></B>\n",
    "<HR>\n",
    "</BODY>\n",
    "</HTML>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# HTML Tags\n",
    "___\n",
    "\n",
    "The purpose of an HTML tags is to provides content for a given webpage. The structure of the tags are start < TAG> and end < /TAG>. The <, >, and / characters are called delimiters, function is to tell the parser that enclosed infomation is an HTML element. When we are scraping infomation we use HTML tags to extract the information.\n",
    "\n",
    "Lets look at a snippet of html.\n",
    "\n",
    "< p>This is some text in a paragraph.< /p>  \n",
    "*note there is a space in the tags the notebook automatically render html.  \n",
    "\n",
    "We can use the < p> < /p> extract text from a paragraph on a webpage. When parsed the infomation will be \"This is some text in a paragraph\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# HTML Tags Examples\n",
    "___\n",
    "There are many different tags categories:\n",
    "- Structure Tags\n",
    "- Head Elements\n",
    "- Body Elements\n",
    "- Paragraph / Line Breaks Tags\n",
    "- Link and Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scraping\n",
    "---\n",
    "Scraping is the act of extracting useful data from a website normally done autonomously. There are different ways to achieve this using Python! Overall favorite is making a request and then parse the HTML response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Useful Libraries\n",
    "---\n",
    "urllib2 /  urllib.request - Used to interact with URLS: opening, authentication, redirects, etc. This library varies between verisons of python 2.7 and 3.6\n",
    "\n",
    "BeautifulSoup - Used to extract information from HTML and XML files.\n",
    "\n",
    "Scrapy - A library that intergrates scraping and crawling, more robust then BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lets take a look at wikipedia page\n",
    "___\n",
    "We can have look at all the data we can collect on a given wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Setting things up!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "webpage = \"https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India\"\n",
    "\n",
    "#This returns HTTPResponse object need to read and  \n",
    "#decode to the see the contents.\n",
    "html_bytes = urllib.request.urlopen(webpage)\n",
    "\n",
    "#This is the soup object that will parse th HTML page\n",
    "soup = BeautifulSoup(html_bytes,\"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to the tags, one can get at the data in the tags\n",
    "# soup.<tag>\n",
    "\n",
    "# Lets get the title of the webpage\n",
    "soup.title\n",
    "\n",
    "#Removes the tags\n",
    "soup.title.string\n",
    "\n",
    "#Here is a list of different tags: https://www.w3schools.com/tags/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More cool stuff with beautiful soup\n",
    "---\n",
    "Lets look into HTML attributes and why they are useful for scraping. We will look at < a href> tag, this tag contains a link on the page. This is an example on how to get all the HTML attribute that is href. Quick side note: href is short for Hypertext REFerence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# To get an HTML Attributes\n",
    "#soup.a\n",
    "\n",
    "# But how do we get the href, have no fear\n",
    "all_links = soup.find_all(\"a\")\n",
    "\n",
    "# Prints all the links that have a href atrributes and filter out bad links\n",
    "href_links = [link.get('href') for link in all_links if link.get('href') is not None]\n",
    "#href_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What if we wanted to scrape data out of a table?\n",
    "---\n",
    "Once again we need to understand what HTML tags we need to work with. HTML uses the < table> to define a table but we need the rows, columns, and header. You guessed it there are tags for that! The tags in question are < th> table header, < tr> is the table row, and < td> is the table cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example how to extract data from a table, this is a specific table on the webpage.\n",
    "data = []\n",
    "cleaned = []\n",
    "\n",
    "# We want to find the table in the HTML: we can do this by using the inspect tool through the\n",
    "# browser. Takes a little time but it does the trick. There also chrome plugins that will aid in\n",
    "# this proces.\n",
    "table = soup.find(\"table\", attrs={\"class\" : \"wikitable\"})\n",
    "\n",
    "# One can also use CSS Selectors.\n",
    "#table=soup.select('th , td , #firstHeading')\n",
    "\n",
    "table_body = table.find('tbody')\n",
    "\n",
    "table_headers = []\n",
    "header_row = []\n",
    "rows = table_body.find_all('tr')\n",
    "\n",
    "for header in table.find_all('th'):\n",
    "    if header.get('scope') == 'col':\n",
    "        table_headers.append(header.text.replace('\\n',' '))\n",
    "    \n",
    "    if header.get('scope') == 'row':\n",
    "        header_row.append(header.text)\n",
    "\n",
    "        \n",
    "# We need to iterate over the then the cols which is the cell data. This throw it into a list     \n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [column_data.text.strip().replace('\\n',' ') for column_data in cols]\n",
    "    \n",
    "    if cols != []:\n",
    "        data.append([ele for ele in cols if cols])\n",
    "\n",
    "# This to make the table look like what it looks like on the wiki page.\n",
    "counter=0\n",
    "for row_info in data:\n",
    "    if row_info != []:\n",
    "        row_info.insert(0,header_row[counter])\n",
    "        row_info.insert(0,counter)\n",
    "        counter += 1\n",
    "\n",
    "wiki_table = pd.DataFrame.from_records(data,columns=table_headers)\n",
    "wiki_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wonder if we can download images? Yes WE CAN!\n",
    "___\n",
    "Python is wonderful right! Let's take a look a simple way to grab images from a webage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_path(file_name):\n",
    "    '''\n",
    "        Return a the file path where the image will be saved to.\n",
    "        ~/csci303/lectures/scraped_imgs/file_name if using the init file structure \n",
    "        \n",
    "        or the ~/path_of_notebook/scraped_imgs/filename\n",
    "    '''\n",
    "    return os.getcwd()+\"/scraped_imgs/\"+file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This a quick example how to download images for a webpage.\n",
    "    Follows the same pattern we have been doing, grab all the \n",
    "    the tags in question for this example we are looking at the\n",
    "    img tag.\n",
    "'''\n",
    "\n",
    "domain = \"https://en.wikipedia.org\"\n",
    "\n",
    "delimiter = '/'\n",
    "outside_domain_token = '//'\n",
    "\n",
    "#First we need to get the src URL for the images and throw out dups.\n",
    "img_links = list(set([link['src'] for link in soup.find_all('img')]))\n",
    "\n",
    "for link in img_links:\n",
    "    \n",
    "    #parsing the file_name from the URL\n",
    "    file_name = link.split(delimiter)[-1]\n",
    "    \n",
    "    # We have to check if the file lives in domain or not.\n",
    "    #\n",
    "    # not in domain //some_url.com/sdfds/file\n",
    "    # /dsfsd/fds/filename if file lives in the domain \n",
    "    \n",
    "    if outside_domain_token in link:\n",
    "        url = \"https:\" + link\n",
    "    else:\n",
    "        url = domain + link\n",
    "        \n",
    "    urllib.request.urlretrieve(url,file_path(file_name))\n",
    "    print(\"Downloaded: \" + file_name)\n",
    "    print(\"File path: \" + file_path(file_name) + \"\\n\")\n",
    "    \n",
    "    #To slow down the request rate\n",
    "    time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
