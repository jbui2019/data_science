{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### CSCI 303\n",
    "# Introduction to Data Science\n",
    "<p/>\n",
    "### Web Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Lecture\n",
    "---\n",
    "- Web Scraping Review\n",
    "- What is Web Crawling\n",
    "- In Class Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Setting things up!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "webpage = \"https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India\"\n",
    "\n",
    "#This returns HTTPResponse object need to read and  \n",
    "#decode to the see the contents.\n",
    "html_bytes = urllib.request.urlopen(webpage)\n",
    "\n",
    "#This is the soup object that will parse th HTML page\n",
    "soup = BeautifulSoup(html_bytes,\"html5lib\")\n",
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to the tags, one can get at the data in the tags\n",
    "# soup.<tag>\n",
    "\n",
    "# Lets get the title of the webpage\n",
    "soup.title\n",
    "\n",
    "#Removes the tags\n",
    "soup.title.string\n",
    "\n",
    "#Here is a list of different tags: https://www.w3schools.com/tags/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# To get an HTML Attributes\n",
    "#soup.a\n",
    "\n",
    "# But how do we get the href, have no fear\n",
    "all_links = soup.find_all(\"a\")\n",
    "\n",
    "# Prints all the links that have a href atrributes and filter out bad links\n",
    "href_links = [link.get('href') for link in all_links if link.get('href') is not None]\n",
    "\n",
    "for links in href_links:\n",
    "    print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Web Crawling\n",
    "___\n",
    "A spider is normally an automated script that browses the web in a logical manner. This process is web crawling. This is what search engines do just on a large scale like a fleet of spiders! \n",
    "\n",
    "Overall the task of the spider is to collect webpage content and more links! \n",
    "\n",
    "Some things to take into account. \n",
    "- The rate of the requests, to many requests will not be good. \n",
    "- The spider with cause a denial of service ... What the spider is allowed to visit (robots.txt). \n",
    "- Then not to let the spider get outside the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Robots.txt\n",
    "---\n",
    "\n",
    "The purpose is a set of rules for a web robot (spider, crawler...etc). \n",
    "\n",
    "For more information: http://www.robotstxt.org/robotstxt.html\n",
    "\n",
    "To find the robots.txt for a given domain, it is located at: https://domain.com/robots.txt (e.g., try to go view Google's robot file: https://www.google.com/robots.txt)\n",
    "\n",
    "Here is an example of what the text file looks like.  \n",
    "\n",
    "User-agent some robot or * denote all robots  \n",
    "Disallow: /cgi-bin/  \n",
    "Disallow: /tmp/  \n",
    "Disallow: /~joe/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to read the Robots.txt\n",
    "---\n",
    "To exclude all robots from the entire server  \n",
    "User-agent: *  \n",
    "Disallow: / \n",
    "\n",
    "To allow all robots complete access or the absences of the file.  \n",
    "User-agent: *  \n",
    "Disallow:  \n",
    "\n",
    "The Disallow tells you where your robot cannot visit /tmp/. Your robot cannot visit /tmp and any child directory of the disallowed folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
