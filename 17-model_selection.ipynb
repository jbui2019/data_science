{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### CSCI 303\n",
    "# Introduction to Data Science\n",
    "<p/>\n",
    "### 17 - Model Selection\n",
    "\n",
    "![parameter search](psearch.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Lecture\n",
    "---\n",
    "Model selection in supervised learning\n",
    "- cross validation\n",
    "- parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Data vs Test Data\n",
    "---\n",
    "Recall in a previous lecture, we saw that we could essentially reduce our training error to zero, with a complex enough model.\n",
    "\n",
    "Training error:\n",
    "\n",
    "- Tells us about our approximation power\n",
    "- Doesn't really tell us about prediction quality!\n",
    "\n",
    "We need a way to test performance on previously unseen data:\n",
    "\n",
    "- Typically hold out some data points as a *test set*\n",
    "- Measure performance score on test set to compare models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The test score gives us a useful way to evaluate how well our model *generalizes* to data not included in our training set.\n",
    "\n",
    "By training with different models (e.g., degree 1 versus degree 2, 3, etc. polynomial) and examining the test score, we can hope to minimize both underfitting and overfitting.\n",
    "\n",
    "Next we'll explore how to improve on this basic mechanism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross Validation\n",
    "---\n",
    "The goal of cross validation is to make better use of our (usually limited) training data and to reduce bias introduced by the training/test split itself.\n",
    "\n",
    "The basic idea is to utilize *multiple* training/test splits and then compute a composite score from each individual run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## *k*-Fold Cross Validation\n",
    "---\n",
    "The most popular form of cross validation:\n",
    "\n",
    "- Choose a fixed value for *k* (typically 5 or 10)\n",
    "- Partition the data into k parts (*folds*) of roughly equivalent size\n",
    "- For each of k folds, use the k<sup>th</sup> fold as the test set, and train on all the remaining data\n",
    "- Compute a composite from the k test score results (typically a mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "---\n",
    "For a running example, let's create some sample clusters as we did in the SVM lecture, and do some cross validation with support vector classification (SVC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure repeatability of this notebook \n",
    "# (comment out for new results each run)\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Get some normally distributed samples\n",
    "def sample_cluster(n, x, y, sigma):\n",
    "    x = np.random.randn(n) * sigma + x;\n",
    "    y = np.random.randn(n) * sigma + y;\n",
    "    return np.array([x, y]).T\n",
    "\n",
    "c1 = sample_cluster(50, 1, 0, 0.5)\n",
    "c2 = sample_cluster(50, 0, 1, 0.4)\n",
    "d1 = DataFrame(c1, columns=['x','y'])\n",
    "d2 = DataFrame(c2, columns=['x','y'])\n",
    "d1['class'] = 'a'\n",
    "d2['class'] = 'b'\n",
    "data = d1.append(d2)\n",
    "data.index = pd.RangeIndex(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(c1[:,0], c1[:,1], 'bs', label='a')\n",
    "plt.plot(c2[:,0], c2[:,1], 'r^', label='b')\n",
    "plt.title('The Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross Validation in Scikit-learn\n",
    "---\n",
    "Scikit-learn has a function to perform cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We need to pass it a model object, the inputs of our data, and the targets of our data (in this case, our classes).\n",
    "\n",
    "There are additional parameters to control various aspects of the cross-validation, which we'll explore; the most important one is the `cv` parameter which controls how folds are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear', C = 1)\n",
    "\n",
    "scores = cross_val_score(model, data[['x', 'y']], data['class'],cv = 5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From the above, it appears that we can expect an accuracy of about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when using support vector classification on this problem.\n",
    "\n",
    "It is also useful to think about the variance/standard deviation - how much variation will there be from run to run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Benefits of (*k*-Fold) Cross Validation\n",
    "---\n",
    "- Better score estimates\n",
    "  - All data gets in the test set (once)\n",
    "  - All data gets in the training set (multiple times)\n",
    "- Some information about sensitivity of the data to particular splits\n",
    "- Can use more data in training (e.g., in 5-fold, use 80% of data for each training run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stratification\n",
    "---\n",
    "How you choose folds matters!\n",
    "\n",
    "- Naive approach just takes first k<sup>th</sup> data items as first fold, etc.\n",
    "- In our example, naive approach would mean test set was all one class! \n",
    "\n",
    "Stratified *k*-fold cross validation:\n",
    "\n",
    "- First k<sup>th</sup> of *each class*\n",
    "- Ensures all classes (roughly) equally represented\n",
    "\n",
    "Scikit-learn uses the stratified approach by default *if* the model is a classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Shuffling\n",
    "---\n",
    "- Another technique to use in generating folds\n",
    "- Avoids training on only biased data, e.g., if data was not collected at random\n",
    "- Can be turned on by using a customized `KFold` or `StratifiedKFold` object for the `cv` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "scores = cross_val_score(model, data[['x', 'y']], \n",
    "                         data['class'], cv = StratifiedKFold(5, True))\n",
    "print(scores)\n",
    "print(scores.mean(),scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shuffle-split Cross Validation\n",
    "---\n",
    "An alternative to *k*-fold cross validation:\n",
    "\n",
    "- Randomly samples (without replacement) data points for training set\n",
    "- Remaining points used as test set\n",
    "- Allows size of split independent of number of training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "scores = cross_val_score(model, data[['x', 'y']], \n",
    "                         data['class'], \n",
    "                         cv = ShuffleSplit(20, test_size = 0.2))\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameter Search\n",
    "---\n",
    "Essentially, try different settings of parameters looking for \"sweet spot\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "c_parms = [0.1, 0.25, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "#c_parms = [10, 12, 14, 16, 18, 20, 25, 30, 50, 100, 200, 300, 400, 500]\n",
    "c_scores = []\n",
    "for c in c_parms:\n",
    "    model = SVC(kernel='linear', C=c)\n",
    "    scores = cross_val_score(model, data[['x','y']], data['class'], \n",
    "                             cv = ShuffleSplit(50, test_size = 0.1))\n",
    "    c_scores.append(scores.mean())\n",
    "\n",
    "fig = plt.figure(figsize=[8,6])\n",
    "plt.plot(range(len(c_parms)), c_scores)\n",
    "fig.axes[0].set_xticks(range(len(c_parms)))\n",
    "fig.axes[0].set_xticklabels(c_parms)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid Search\n",
    "---\n",
    "\n",
    "- Tries all combination settings of parameters\n",
    "- Obviously potentially very expensive\n",
    "- Want to do iteratively: narrow in on a range of parameters, then search more finely within those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example, this time using RBFs - we now need to tune `gamma` as well as `C`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "c_parms = [0.1, 0.25, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "g_parms = [0.001, 0.01, 0.1, 1]\n",
    "allscores = []\n",
    "for c in c_parms:\n",
    "    g_scores = []\n",
    "    for g in g_parms:\n",
    "        model = SVC(kernel='rbf', C=c, gamma=g)\n",
    "        scores = cross_val_score(model, \n",
    "                                 data[['x','y']], data['class'], \n",
    "                                 cv = ShuffleSplit(20, test_size = 0.2))\n",
    "        g_scores.append(scores.mean())\n",
    "    allscores.append(g_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[8,6])\n",
    "plt.contour(range(len(g_parms)), range(len(c_parms)), \n",
    "            allscores, colormap='BuRd')\n",
    "fig.axes[0].set_xticks(range(len(g_parms)))\n",
    "fig.axes[0].set_xticklabels(g_parms)\n",
    "plt.xlabel('gamma')\n",
    "fig.axes[0].set_yticks(range(len(c_parms)))\n",
    "fig.axes[0].set_yticklabels(c_parms)\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.title('Cross Validation Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameter Overfitting\n",
    "---\n",
    "We hit our best accuracy at about 0.94 using gamma = 0.1 and C = 200.\n",
    "\n",
    "Can we claim our model is 94% accurate in general?\n",
    "\n",
    "Again we have the potential for a kind of *overfitting* - this time, we did a parameter search which favored a particular set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The solution is to add another layer of validation, by splitting our data further, this time into three groups.\n",
    "\n",
    "We call these groups the training, validation, and test sets.\n",
    "\n",
    "- training: train the model for fixed parameter setting\n",
    "- validation: choose the parameters\n",
    "- test: test for generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size = 0.25)\n",
    "c_parms = [0.1, 0.25, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "g_parms = [0.001, 0.01, 0.1, 1]\n",
    "best = (0, 0)\n",
    "bestscore = 0.0\n",
    "for c in c_parms:\n",
    "    g_scores = []\n",
    "    for g in g_parms:\n",
    "        model = SVC(kernel='rbf', C=c, gamma=g)\n",
    "        scores = cross_val_score(model, train[['x','y']], train['class'], cv = 10)\n",
    "        m = scores.mean()\n",
    "        if m > bestscore:\n",
    "            bestscore = m\n",
    "            best = (c, g)\n",
    "\n",
    "print('Training accuracy:', bestscore)\n",
    "print('Parameters: C =', best[0], ', gamma =', best[1])\n",
    "\n",
    "# Now, fit again using *all* of the training data, and evaluate on test data\n",
    "model = SVC(kernel='rbf', C=best[0], gamma=best[1])\n",
    "model.fit(train[['x','y']], train['class'])\n",
    "print('Testing accuracy:', model.score(test[['x','y']], test['class']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that this time we got very different settings for gamma and C, using less data for training!\n",
    "\n",
    "This implies that our parameter search is very sensitive to the split in the data.\n",
    "\n",
    "This is likely due to the relatively small amount of data we are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid Search with Cross Validation\n",
    "---\n",
    "Because the above operation is so common, scikit-learn has a function for it (of course).\n",
    "\n",
    "Using the GridSearchCV function, you can do a search across a variety of parameters (even including things like different kernel types in SVC).\n",
    "\n",
    "E.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parms = {'C': c_parms, 'gamma': g_parms}\n",
    "\n",
    "model = GridSearchCV(SVC(), parms, cv=10)\n",
    "model.fit(train[['x','y']], train['class'])\n",
    "model.score(test[['x','y']], test['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('Training accuracy:', model.best_score_)\n",
    "print('Best parameters:', model.best_params_)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "height": 768,
   "start_slideshow_at": "selected",
   "theme": "mines",
   "transition": "slide",
   "width": 1024
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
